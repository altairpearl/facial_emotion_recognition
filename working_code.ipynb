{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Facial emotion recogniton using Multi-layer CNN\n",
    "## Author: Muthu Kumaran Manohara\n",
    "## UFID: 9718-4490\n",
    "\n",
    "# The original source code from https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280 \n",
    "# was modified for this project\n",
    "\n",
    "import sys, os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Concatenate, concatenate\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.initializers import  RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv('./fer2013/fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "width, height = 48, 48\n",
    "\n",
    "dp = df['pixels'].tolist()\n",
    "\n",
    "#Preprocessing\n",
    "fb = []\n",
    "for pix_seq in dp:\n",
    "    fc = [int(pix) for pix in pix_seq.split(' ')]\n",
    "    fc = np.asarray(fc).reshape(width, height)\n",
    "    fb.append(fc.astype('float32'))\n",
    "\n",
    "fb = np.asarray(fb)\n",
    "fb = np.expand_dims(fb, -1)\n",
    "\n",
    "#getting labels for training\n",
    "emo = pd.get_dummies(df['emotion']).values\n",
    "\n",
    "#Storing the face\n",
    "np.save('fdata', fb)\n",
    "np.save('flabels', emo)\n",
    "\n",
    "print(\"Preprocessing completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "width, height = 48, 48\n",
    "\n",
    "fb = np.load('./fdata.npy')\n",
    "emo = np.load('./flabels.npy')\n",
    "\n",
    "fb -= np.mean(fb, axis=0)\n",
    "fb /= np.std(fb, axis=0)\n",
    "\n",
    "img_pxls = fb.shape[1]\n",
    "img_wid = img_hgt = np.ceil(np.sqrt(img_pxls)).astype(np.uint8)\n",
    "lbl = df[\"emotion\"].values.ravel()\n",
    "lbl_count = np.unique(lbl).shape[0]\n",
    "\n",
    "# Function for creating zero/ones matrix indicating image label\n",
    "def lbl_encoding(lbl_dense, lbl_class):\n",
    "    num_labels = lbl_dense.shape[0]\n",
    "    offset = np.arange(num_labels) * lbl_class\n",
    "    on_enc = np.zeros((num_labels, lbl_class))\n",
    "    on_enc.flat[[offset + lbl_dense.ravel()]] = 1\n",
    "    return on_enc\n",
    "\n",
    "\n",
    "lbls = lbl_encoding(lbl, lbl_count)\n",
    "lbls = lbls.astype(np.uint8)\n",
    "\n",
    "fb = fb.reshape(fb.shape[0], 48, 48, 1)\n",
    "fb = fb.astype('float32')\n",
    "\n",
    "#splitting into training, validation and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(fb, lbls, test_size=0.1, shuffle = False)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle = False)\n",
    "\n",
    "#saving the test samples to be used later\n",
    "np.save('modified_Xtest', X_test)\n",
    "np.save('modified_ytest', y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our modification, we have created a multi-level CNN network\n",
    "inp = Input(shape=(width, height, 1))\n",
    "block_1 = Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(inp)\n",
    "block_1_1 = Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_1)\n",
    "block_1_2 = BatchNormalization()(block_1_1)\n",
    "block_1_3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(block_1_2)\n",
    "block_1_4 = Dropout(0.25)(block_1_3)\n",
    "\n",
    "block_2 = Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_1_4)\n",
    "block_2_0 = BatchNormalization()(block_2)\n",
    "block_2_1 = Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_2_0)\n",
    "block_2_2 = BatchNormalization()(block_2_1)\n",
    "block_2_3 = Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_2_2)\n",
    "block_2_4 = BatchNormalization()(block_2_3)\n",
    "block_2_5 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(block_2_4)\n",
    "block_2_6 = Dropout(0.25)(block_2_5)\n",
    "\n",
    "fc0_t = Dense(2*2*num_features, activation='relu')(block_2_6)\n",
    "fc0 = Flatten()(fc0_t)\n",
    "\n",
    "block_3 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_2_4)\n",
    "block_3_0 = BatchNormalization()(block_3)\n",
    "block_3_1 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_3_0)\n",
    "block_3_2 = BatchNormalization()(block_3_1)\n",
    "block_3_3 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_3_2)\n",
    "block_3_4 = BatchNormalization()(block_3_3)\n",
    "block_3_5 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_3_4)\n",
    "block_3_6 = BatchNormalization()(block_3_5)\n",
    "block_3_7 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(block_3_6)\n",
    "block_3_8 = Dropout(0.25)(block_3_7)\n",
    "\n",
    "fc1_t = Dense(2*2*2*num_features, activation='relu')(block_3_8)\n",
    "fc1 = Flatten()(fc1_t)\n",
    "\n",
    "block_4 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_3_8)\n",
    "block_4_0 = BatchNormalization()(block_4)\n",
    "block_4_1 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_4_0)\n",
    "block_4_2 = BatchNormalization()(block_4_1)\n",
    "block_4_3 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_4_2)\n",
    "block_4_4 = BatchNormalization()(block_4_3)\n",
    "block_4_5 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_4_4)\n",
    "block_4_6 = BatchNormalization()(block_4_5)\n",
    "block_4_7 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(block_4_6)\n",
    "block_4_8 = Dropout(0.25)(block_4_7)\n",
    "\n",
    "fc2_t = Dense(2*2*2*num_features, activation='relu')(block_4_8)\n",
    "fc2 = Flatten()(fc2_t)\n",
    "\n",
    "block_5 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_4_8)\n",
    "block_5_0 = BatchNormalization()(block_5)\n",
    "block_5_1 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_5_0)\n",
    "block_5_2 = BatchNormalization()(block_5_1)\n",
    "block_5_3 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_5_2)\n",
    "block_5_4 = BatchNormalization()(block_5_3)\n",
    "block_5_5 = Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1))(block_5_4)\n",
    "block_5_6 = BatchNormalization()(block_5_5)\n",
    "block_5_7 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(block_5_6)\n",
    "block_5_8 = Dropout(0.25)(block_5_7)\n",
    "c0 = Flatten()(block_5_8)\n",
    "\n",
    "fc3_t = Dense(2*2*2*num_features, activation='relu')(block_5_8)\n",
    "fc3 = Flatten()(fc3_t)\n",
    "\n",
    "out_int = concatenate([c0, fc0, fc1, fc2, fc3])\n",
    "block_x = Flatten()(out_int)\n",
    "\n",
    "out = Dense(lbl_count, activation='softmax')(block_x)\n",
    "\n",
    "model = Model(inputs=[inp], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 48, 48, 64)   640         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 48, 48, 64)   36928       conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 48, 48, 64)   256         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D) (None, 24, 24, 64)   0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 24, 24, 64)   0           max_pooling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 24, 24, 128)  73856       dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 24, 24, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 24, 24, 128)  147584      batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 24, 24, 128)  512         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 24, 24, 128)  147584      batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 24, 24, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 24, 24, 256)  295168      batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 24, 24, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 24, 24, 256)  590080      batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 24, 24, 256)  1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 24, 24, 256)  590080      batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 24, 24, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 24, 24, 256)  590080      batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 24, 24, 256)  1024        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D) (None, 12, 12, 256)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 12, 12, 256)  0           max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 12, 12, 256)  590080      dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 12, 12, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 12, 12, 256)  590080      batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 12, 12, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 12, 12, 256)  590080      batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 12, 12, 256)  1024        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 12, 12, 256)  590080      batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 12, 12, 256)  1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D) (None, 6, 6, 256)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 6, 6, 256)    0           max_pooling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 6, 6, 256)    590080      dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 6, 6, 256)    1024        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 6, 6, 256)    590080      batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 6, 6, 256)    1024        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 6, 6, 256)    590080      batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 6, 6, 256)    1024        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 6, 6, 256)    590080      batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 6, 6, 256)    1024        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D) (None, 3, 3, 256)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D) (None, 12, 12, 128)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 3, 3, 256)    0           max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 12, 12, 128)  0           max_pooling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 12, 12, 256)  33024       dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 12, 12, 512)  131584      dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 6, 6, 512)    131584      dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 3, 3, 512)    131584      dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 2304)         0           dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 36864)        0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 73728)        0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 18432)        0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 4608)         0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 135936)       0           flatten_32[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "                                                                 flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 135936)       0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 7)            951559      flatten_34[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,586,055\n",
      "Trainable params: 8,579,015\n",
      "Non-trainable params: 7,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 908.375 steps, validate on 3230 samples\n",
      "Epoch 1/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 2.5208 - accuracy: 0.2157\n",
      "Epoch 00001: val_loss improved from inf to 1.89475, saving model to Model.01-0.2505.hdf5\n",
      "909/908 [==============================] - 58s 64ms/step - loss: 2.5201 - accuracy: 0.2158 - val_loss: 1.8947 - val_accuracy: 0.2505\n",
      "Epoch 2/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.9763 - accuracy: 0.2226 - ETA: 0s - loss: 1.9769 \n",
      "Epoch 00002: val_loss improved from 1.89475 to 1.86220, saving model to Model.02-0.2783.hdf5\n",
      "909/908 [==============================] - 56s 61ms/step - loss: 1.9763 - accuracy: 0.2225 - val_loss: 1.8622 - val_accuracy: 0.2783\n",
      "Epoch 3/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.9011 - accuracy: 0.2325 - ETA: 15s - loss: 1.9104 - ac - ETA: 14s - loss: 1.9092 - accuracy:  - ETA: 13s - loss: 1.90 - ETA: 12s - loss: 1.9071 - accurac - ETA: - ETA: 7s - loss: 1.9053 - ac - ETA: 6s - loss: 1.9053 -  - ETA: 3s - loss: 1.901 - ETA: 3s - loss: 1.900 - ETA: 2s - loss: 1.9005 -  - ETA: 0s - loss: 1.9014 - accuracy - ETA: 0s - loss: 1.9011  - ETA: 0s - loss: 1.9012 - accuracy: 0.2324\n",
      "Epoch 00003: val_loss improved from 1.86220 to 1.78157, saving model to Model.03-0.2966.hdf5\n",
      "909/908 [==============================] - 57s 62ms/step - loss: 1.9014 - accuracy: 0.2322 - val_loss: 1.7816 - val_accuracy: 0.2966\n",
      "Epoch 4/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.8532 - accuracy: 0.2410 - ETA: 46s -  - ETA: - ETA: 42s - loss: 1.865 - ETA: 41s - loss: 1.8650 - accur - ETA: - ETA: 35s - loss: 1.8616 - accur - ETA: 34s - loss: 1.8622 - accur - ETA: 34s - loss: 1.8602 - accuracy:  - ETA - ETA: 31s - loss: - ETA: 30s - loss: 1.8642 - accuracy: 0.236 - ETA: 30s - loss: 1.8642 - accuracy: 0.2 - ETA: 29s - loss: 1.8644 - accuracy: 0.23 - ETA: 29s - loss: 1.8643 - accuracy: 0.23 - ETA: 29s - loss: 1.8648 - acc - ETA: 28s - loss: 1.8655 - accurac - ETA: 28s - loss: 1.8654 - accuracy: 0.2 - ETA: 28s - loss: 1.8650 - accuracy: - ETA: 27s - loss: 1.8641 - accuracy: 0.2 - ETA: - ETA: 22s - loss: 1.8639 - accuracy:  - - ETA: 19s - loss: 1.8598 - ETA: 18s - loss: 1.8586 -  - ETA: 17s - loss: 1.8586 - accuracy - ETA: 16s - loss: 1 - ETA: 15s - loss: 1.85 - - ETA: 7s - los - ETA:  - ETA: 0s - loss: 1.8526 - accura - ETA: 0s - loss: 1.8533 - accuracy: 0.2410\n",
      "Epoch 00004: val_loss did not improve from 1.78157\n",
      "909/908 [==============================] - 58s 64ms/step - loss: 1.8534 - accuracy: 0.2411 - val_loss: 1.8249 - val_accuracy: 0.2759\n",
      "Epoch 5/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.8169 - accuracy: 0.2514 - ETA: 53s - loss: 1.8097 - accurac - ETA: 53s - loss: 1.8131 - accuracy: 0. - ETA: 52s - loss: 1.8160 - accuracy: 0. - ETA: 52s - loss: 1.8177 - accuracy: 0.25 - ETA: 52s - loss: 1.8196 - accuracy: 0.251 - ETA: 52s - loss: 1.8178 - accuracy:  - ETA: 51s - loss: 1.8151 - accuracy:  - ETA: 51s - loss: 1.8139 - accuracy: 0 - ETA: 51s - loss: 1.8159 - accuracy: 0.253 - ETA: 51s - loss: 1.8165 - ETA: 49s - loss: 1.8176 - accuracy: 0.256 -  - ETA: 46 - ETA: 41s - l - ETA: 37s - loss: - ETA: 35s - loss: 1.8220 - accuracy: 0 - ETA: 35s - loss: 1.8217 -  - ETA: 34s - loss - ETA: 32s - loss: 1.8264 - accurac - ETA: 31s - loss: 1.8264 - ETA: 30s - loss: 1.8247 - accuracy: 0.251 - ETA: 30s - loss: 1.8248 - accuracy: 0.25 - ETA: 3 - ETA: 28s - loss: 1.8263 - accuracy: 0.250 - ETA: 28s - loss: 1.8265 - accuracy: 0 - ETA: 27s - loss: 1.825 - ETA: 26s - loss: 1.8248 - accurac - ETA: 25s - loss: 1.8242 - accuracy: 0.251 - ETA: 25s - loss: 1.8244 - accuracy: 0.251 - ETA: 25s - loss: 1 - ETA: -  - ETA: 19s - loss: 1.8200 - ac - ETA: 18s - loss: 1.8209 - acc - ETA: 17s - loss: 1.8195 - accuracy:  - ETA: 14s - loss: 1.8198 - accuracy: 0.249 - ETA: 14s - loss: 1.8197 - accuracy: 0.249 - ETA: 14s -  - ETA: 12s - loss: 1.8 - ETA: 8s - loss: 1.8 - ETA:  - ETA: 0s - loss: 1.8168 - accuracy: 0.\n",
      "Epoch 00005: val_loss did not improve from 1.78157\n",
      "909/908 [==============================] - 60s 66ms/step - loss: 1.8167 - accuracy: 0.2515 - val_loss: 1.8060 - val_accuracy: 0.2994\n",
      "Epoch 6/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.7858 - accuracy: 0.2646 - ETA: 54s - loss: 1.8149 - acc - ETA: 53s - loss: 1.8089 - - ETA - ETA: 49s - loss: 1.8037 - a - ET - ETA: 46s - loss: 1.8043 - accura - ETA: 45s - loss: 1.8033 - accuracy: 0 - ETA: 45s - - ETA: 40s - loss: - ETA: 39s - loss: 1.8018 - accu - ETA: 3 - ETA: 36s - loss: 1.7988 - accuracy:  - ETA: 35s - loss: 1.7983 - accura - ETA: 35s - loss: 1.7969 - accu - ETA: 34s - loss: 1.7978  - ETA: 33s - loss: 1.7985  - ETA: 31s - loss: 1.7969 - accuracy: 0. - ETA: 31s - loss: 1.7963 - accuracy - ETA: 31s - loss: 1.796 - - ETA - ETA: 1s - loss: 1.7868 - accuracy: 0.26 - ETA: \n",
      "Epoch 00006: val_loss improved from 1.78157 to 1.72506, saving model to Model.06-0.3201.hdf5\n",
      "909/908 [==============================] - 61s 67ms/step - loss: 1.7859 - accuracy: 0.2646 - val_loss: 1.7251 - val_accuracy: 0.3201\n",
      "Epoch 7/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.7623 - accuracy: 0.2762 - ETA\n",
      "Epoch 00007: val_loss improved from 1.72506 to 1.71867, saving model to Model.07-0.3226.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.7623 - accuracy: 0.2762 - val_loss: 1.7187 - val_accuracy: 0.3226\n",
      "Epoch 8/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.7412 - accuracy: 0.2829\n",
      "Epoch 00008: val_loss did not improve from 1.71867\n",
      "909/908 [==============================] - 62s 69ms/step - loss: 1.7413 - accuracy: 0.2828 - val_loss: 1.7531 - val_accuracy: 0.3365\n",
      "Epoch 9/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.7247 - accuracy: 0.2930 ETA: 1s - loss: 1.7 - ETA: 1s\n",
      "Epoch 00009: val_loss did not improve from 1.71867\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.7249 - accuracy: 0.2929 - val_loss: 1.8274 - val_accuracy: 0.3223\n",
      "Epoch 10/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.7052 - accuracy: 0.3054 - ETA: 56s - loss: 1.7100 - accuracy: 0.30 - ETA: 56s - loss: 1.7142 - accurac - ETA: 55s - loss: 1.7083 - accuracy: 0.310 - ETA: 55s - loss: 1.7078 - accuracy - ETA: 54s - loss: 1.7107 - accuracy: 0.306 - ETA: 54s - loss: 1.7107 - accurac - ETA: 54s - loss: 1.7057 - ac - ETA: 53s - loss: 1.7095 - accuracy: 0 - ETA: 52s - loss: 1.7064 - accura - ETA: 52s - loss: 1.70 - ETA: 50s - loss: 1.6978 - a - ETA: 49s - loss: 1.7019 -  -  - ETA: 43s - loss: 1.7089 - accuracy: 0.308 - ETA: 43s - loss: 1.7086 - accuracy:  - ETA: 42s - loss: 1.710 - ETA: 41s - ETA: 28s - loss: 1.7105 -  - - ETA: 24s - loss: 1.7094 - accuracy: 0. - ETA: 24s - loss: 1.7100 - accuracy: 0.302 - ETA: 23s - loss: 1.7097 - accu - ETA: 20s - loss: 1.7073 - ac - ETA: 19s - loss: 1.7072 - accu - ETA: 18s - loss: 1.7075 - acc - ETA: 17s - lo - ETA: 15s - loss: - ETA: 14s - loss: 1 - ETA: 12s - loss: 1.7063 - accura - ETA: 11s - loss: 1.7069 - accura - ETA: 11s - loss: 1.7064 - accu - ETA: 10s - loss: 1.7055 - accuracy: 0 - ETA: 10s - - E - ETA: 3s - loss: 1.7055 \n",
      "Epoch 00010: val_loss improved from 1.71867 to 1.65854, saving model to Model.10-0.3551.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.7051 - accuracy: 0.3056 - val_loss: 1.6585 - val_accuracy: 0.3551\n",
      "Epoch 11/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.6809 - accuracy: 0.3243 - ETA: 56s - loss: 1.6705 - a - ETA: 54s - loss: 1.6677 - accuracy: 0.32 - ETA: 54s - loss: 1.6704 - accuracy: - ETA: 53s - los - ETA: 51s - loss: 1.6823 - accuracy: 0.321 - ETA: 51s - loss: 1.6825 - accuracy: 0.323 - ETA: 51s - loss: 1.6828 -  - ETA: 50s - loss: 1.6844 - accur - ETA: 49s - loss: 1.6874 - accuracy: 0.3 - ETA: 49s - loss: 1.6877 -\n",
      "Epoch 00011: val_loss improved from 1.65854 to 1.61332, saving model to Model.11-0.3675.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.6810 - accuracy: 0.3243 - val_loss: 1.6133 - val_accuracy: 0.3675\n",
      "Epoch 12/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.6619 - accuracy: 0.32550- ETA: 14s  - ETA: 12s - loss: 1.6657 - accuracy: - ETA: 12s - loss: 1.6654 - ETA: 10s - loss: 1.6656 - accurac - ETA: 10s - loss: 1.6650 - accuracy: 0. - ETA: 9s - loss: 1.6649 - - ETA: 9s - loss: - ETA: 4s - loss: 1.663 - ETA:  - ETA: 0s - loss:\n",
      "Epoch 00012: val_loss did not improve from 1.61332\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.6618 - accuracy: 0.3256 - val_loss: 1.6594 - val_accuracy: 0.3765\n",
      "Epoch 13/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.6279 - accuracy: 0.3540 ETA: 0s - loss:\n",
      "Epoch 00013: val_loss did not improve from 1.61332\n",
      "909/908 [==============================] - 64s 70ms/step - loss: 1.6280 - accuracy: 0.3540 - val_loss: 1.6718 - val_accuracy: 0.3889\n",
      "Epoch 14/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.5855 - accuracy: 0.3756 ETA: 0s - loss: 1.5858 - accu\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.61332\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.5857 - accuracy: 0.3755 - val_loss: 1.6790 - val_accuracy: 0.4022\n",
      "Epoch 15/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.5454 - accuracy: 0.3906\n",
      "Epoch 00015: val_loss improved from 1.61332 to 1.52821, saving model to Model.15-0.4409.hdf5\n",
      "909/908 [==============================] - 64s 70ms/step - loss: 1.5456 - accuracy: 0.3905 - val_loss: 1.5282 - val_accuracy: 0.4409\n",
      "Epoch 16/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.5015 - accuracy: 0.4101 - ETA: 23s - loss: 1.5058 - ac - ETA: 22s - loss: 1.50 - ETA: 21s - lo - ETA: 19s - loss: 1.5061 - accur - ETA: 18s - loss: 1.5048 - accuracy: 0.4 - ET - ETA: 12s - loss: 1.5043 - accura - ETA: 11s -  - ETA: 9s - ETA: 2s - loss: 1.5 - ETA: 2s - loss: 1.5021 - accuracy: 0.40 - ETA: 2s - loss: 1.5022 - accuracy - ETA: 1s - loss: 1.5022 - accuracy: 0. - ETA: 1s - loss: - ETA: 0s - loss: 1.5\n",
      "Epoch 00016: val_loss improved from 1.52821 to 1.43624, saving model to Model.16-0.4644.hdf5\n",
      "909/908 [==============================] - 63s 69ms/step - loss: 1.5012 - accuracy: 0.4101 - val_loss: 1.4362 - val_accuracy: 0.4644\n",
      "Epoch 17/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.4628 - accuracy: 0.4326 - ETA: 58s - loss: 1.4313 - accuracy: 0.4 - ETA: 58s - loss: 1.4585 - accuracy: - ETA: 57s - loss: 1.4592 - - E - ETA: 53s - loss: 1.454 - ETA: 51s  - ETA: 50s - los - ETA: 4 - ETA: 46s - loss: 1.4478 - acc\n",
      "Epoch 00017: val_loss improved from 1.43624 to 1.35443, saving model to Model.17-0.4929.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.4628 - accuracy: 0.4324 - val_loss: 1.3544 - val_accuracy: 0.4929\n",
      "Epoch 18/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.4261 - accuracy: 0.4491 ETA: 4s - loss: 1.4273 - accuracy: 0. - ETA: 4s - loss: 1.4274  - ETA: 4s - ETA - ETA: 0s - loss: 1.4255 - accuracy\n",
      "Epoch 00018: val_loss did not improve from 1.35443\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.4262 - accuracy: 0.4489 - val_loss: 1.3820 - val_accuracy: 0.4861\n",
      "Epoch 19/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.3942 - accuracy: 0.4617   ETA: 1:00 - loss: 1.4335 - accuracy: 0 - ETA: 59s - loss: 1.4549 - - ETA: 57s - lo - ETA: 55s - loss: 1.4368 - accuracy: - ETA: 55s - loss: 1.4374 - accura - ETA: 54s - loss: 1 - ETA: 5 - ETA: 47s - loss: 1.4269 -  - ETA: 46s - los - ET - ETA: 36s - loss: 1.4185 - accura - ETA: 35s -  - ETA: 33s - loss: 1.4136 - a - ETA: 23s - loss: 1.4020 - accurac - ETA: 23s - loss: 1.4019 - accu - ETA: 22s - loss: 1.4014 - accur - ETA: 2 - ETA: 19s - loss: 1.3950 - accuracy - ETA: 18s - loss: 1.3953 - accuracy: 0.462 - ETA: 18s - loss: 1.3953 - accurac - ETA: 18s - loss: 1.3954 - accuracy: - ETA: 17s - loss: 1.3950 - accuracy: 0. - ETA: 17s - l - ETA: 12s - loss: 1.3958 - accuracy: 0.4 - ETA:  - ETA: 2s - loss: 1.3951 - accuracy: 0.46 - ETA: 2s - loss: 1.395 - ETA: 1s - loss: 1.3 - ETA: 1s\n",
      "Epoch 00019: val_loss improved from 1.35443 to 1.31475, saving model to Model.19-0.5105.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.3943 - accuracy: 0.4617 - val_loss: 1.3147 - val_accuracy: 0.5105\n",
      "Epoch 20/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.3547 - accuracy: 0.4794 - ETA: 24s - - ETA: 16s - loss: 1.3597 - accuracy: 0. - \n",
      "Epoch 00020: val_loss improved from 1.31475 to 1.31451, saving model to Model.20-0.4991.hdf5\n",
      "909/908 [==============================] - 62s 69ms/step - loss: 1.3546 - accuracy: 0.4793 - val_loss: 1.3145 - val_accuracy: 0.4991\n",
      "Epoch 21/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.3273 - accuracy: 0.4918 - ETA: 58s - loss: 1.3123 -  - ETA: 56s - loss: 1.30 - ETA: 54s - loss: 1.3074 - ac - ETA: 53s - loss: 1.3239 - accur - ETA: 53s - loss: 1.3249 - accurac - ETA: 52s - loss: 1.3144 - accuracy: 0.496  - ETA: 45s - loss: 1.3175 - accuracy: 0.493 - ETA: 45s - loss: 1.3178 - accuracy: 0 - ETA: 45s - loss: 1.3181 - accuracy: 0.492 - ETA: 45s - loss - ETA: 43s - loss: 1.320 - ETA: 33s - loss: 1. - ETA:  - ETA: 29s - loss: 1.326 - ETA: 27s - loss: 1.3273 - accur - - ETA: 24s - loss: 1.3300 - accurac - ETA: 23s - loss: 1.3306 - accuracy:  - ETA: 23s - loss: 1.3300 - accura - ETA: 22 - ETA: 20s - loss - ETA: 18s - loss: 1.327 - ETA: 17s - loss: 1.32 - ETA - ETA: 4s - los - ETA: 2s - loss: 1.3265 - accu - ETA: 1s - - ETA: 0s - loss: 1.3267 \n",
      "Epoch 00021: val_loss improved from 1.31451 to 1.27983, saving model to Model.21-0.5238.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.3271 - accuracy: 0.4918 - val_loss: 1.2798 - val_accuracy: 0.5238\n",
      "Epoch 22/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.2967 - accuracy: 0.5041 - ETA: 57s - loss: 1.3451 - ETA: 48s - loss: 1.3145 - - ETA: 47s - loss: 1.3207  - ETA: 43s - loss: 1.3057 - accur - ETA: 43s - lo - ETA: 41s - loss: 1.3044 - accuracy: 0 - ETA: 40s - loss: 1.3039 - accuracy: 0  - ETA: 34s - loss: 1.3020 - accuracy: 0. - ETA: 34s - loss: 1.30 - ETA: 32s - loss: 1 - ETA: 28s - loss: 1.3005 - accura - ETA: 27s - loss: 1.3008 - accuracy: 0. - ETA: 27s - loss: 1.3015 - accu - ETA: 26s - loss: 1.3003 - a - ETA: 25s - loss: 1.3016 - ac - - ETA: 16s - loss: 1.2980 - a - ETA: 15s - loss: 1.2989 - a - ETA: 14s - loss: 1.3000 - - ETA: 13s - loss: 1.3001 - accura - ETA: 12s - loss: 1.3001 - accuracy: 0 - ETA: 12s - loss: 1.299 - ETA: 1s - loss: 1.2953 - ac - ETA: 1s\n",
      "Epoch 00022: val_loss improved from 1.27983 to 1.27691, saving model to Model.22-0.5062.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.2965 - accuracy: 0.5042 - val_loss: 1.2769 - val_accuracy: 0.5062\n",
      "Epoch 23/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.2706 - accuracy: 0.5143 - ETA: 54s - loss: 1.259 - ETA: 52s - loss: 1.2643 - accuracy: 0.5 - ETA: 52s - lo -  - ETA: 46s - l - ETA: 0s - loss: 1.2708 - accura\n",
      "Epoch 00023: val_loss improved from 1.27691 to 1.24419, saving model to Model.23-0.5251.hdf5\n",
      "909/908 [==============================] - 63s 69ms/step - loss: 1.2707 - accuracy: 0.5143 - val_loss: 1.2442 - val_accuracy: 0.5251\n",
      "Epoch 24/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.2455 - accuracy: 0.5233 - ETA: 55s - loss: 1.2532 - acc - ETA: 54s - loss: 1 -  - ETA: 50s - loss: 1.2652 - accuracy:  - ETA: 50s - loss: 1.2641 - accuracy: 0.5 - ETA: 49s - loss:  - ETA: 48s - loss: 1.2663 - ac - - ETA: 44s  - ETA:  - ETA: 33s - loss: 1.2649 - accura - ETA: 30s - loss: 1.2649 -  - ETA: 14s - loss: 1.2499 - accurac - E - ETA: 6s - loss: 1.2 - ETA:  - ETA\n",
      "Epoch 00024: val_loss improved from 1.24419 to 1.22847, saving model to Model.24-0.5344.hdf5\n",
      "909/908 [==============================] - 62s 69ms/step - loss: 1.2454 - accuracy: 0.5234 - val_loss: 1.2285 - val_accuracy: 0.5344\n",
      "Epoch 25/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.2263 - accuracy: 0.5305 - ETA: 57s -  - ETA: 48s - loss: 1.2351 - accuracy: 0.52 - ETA: 48s - loss: 1.2357 - accuracy: 0.52 - ETA: 48s - loss: 1.2369 - accuracy:  - ETA: 47s - loss: 1.2367 - accuracy: 0 - ETA: 47s - loss: 1.2405 - accuracy:  - ETA: 46s - loss: 1.2424 - accuracy:  - ETA: 46s - loss: 1.2430 - accuracy:  - ETA: 46s - loss: 1.2441 - accuracy:  - ETA: 45s - loss: 1.2450 - accuracy: 0.522 - ETA: 45s - loss: 1. - ETA: 43s - loss: 1.2465 - accuracy: 0.519 - ETA: 43s - loss: 1 - ETA: 42s - loss: 1.2483 - accuracy: 0 - ETA: 41s - loss: 1.2480 - accuracy: - ETA: 41s - loss: 1.2490 - accuracy: 0.51 - ETA - ETA: 14s - loss: 1.2322 - accuracy: 0.52 - ETA: 14s - loss: 1.2320 - accuracy - ETA: 14s - loss: 1.232 - ETA: 12s\n",
      "Epoch 00025: val_loss improved from 1.22847 to 1.22082, saving model to Model.25-0.5387.hdf5\n",
      "909/908 [==============================] - 63s 69ms/step - loss: 1.2267 - accuracy: 0.5303 - val_loss: 1.2208 - val_accuracy: 0.5387\n",
      "Epoch 26/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.2062 - accuracy: 0.5396 - ETA: 54s - loss: 1.1850 - accuracy: 0.54 - ETA: 54s - loss: 1 - ETA: 53s - loss: 1. - ETA: 45s - loss: 1.1920 - a - ETA: 44s - loss: 1.1955 - accuracy: 0.548 - ETA: 44s - loss: 1.1965 - accuracy: 0 - ETA: 44s - loss: 1.1986 - accuracy: 0. - ETA:  - ETA: 41s - loss: 1.1917 - a - ETA: 40s - loss: 1.1927 - accuracy: 0.\n",
      "Epoch 00026: val_loss improved from 1.22082 to 1.15242, saving model to Model.26-0.5684.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.2059 - accuracy: 0.5398 - val_loss: 1.1524 - val_accuracy: 0.5684\n",
      "Epoch 27/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.1883 - accuracy: 0.5479  - ETA: 57 - ETA: 54s - loss: 1 - ETA: 34s - loss: 1.1837 - accu - ETA: 3 - ETA: 31s - loss: 1.182 - ETA: - ETA: 21s - loss: 1.1849 - accuracy - ETA: 21s - loss: 1.1869 - accuracy: 0. - ETA: 20s - loss: 1.1877 - accuracy: 0.548 - ETA: 20s - loss: 1.1879 - a - ETA: 19s -  - ETA - ETA: 2s - l\n",
      "Epoch 00027: val_loss did not improve from 1.15242\n",
      "909/908 [==============================] - 61s 67ms/step - loss: 1.1888 - accuracy: 0.5478 - val_loss: 1.1726 - val_accuracy: 0.5551\n",
      "Epoch 28/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.1758 - accuracy: 0.5541   ETA: 1:00 - loss: 1.0962 - accuracy - ETA: 59s - loss: 1.1158 - ac - ETA: 57s - loss: 1.1868 - a - ETA: 56s - loss: 1.1525 - ETA: 26s - lo - ETA: 24s - loss: 1.1790 - accuracy: 0. - ETA: 24s - loss: - ETA: 22s - loss: 1.1801 - accuracy: 0.551 - ETA: 22s - loss: 1.1804 - accur - ETA: 21s - loss: 1.1786 - accuracy: 0.55 - ETA: 21s - loss: 1.1785 - accur - ETA: 20s - loss: 1.1779 -  - ETA: 19s - loss: 1. - ETA: 18s - loss: 1.1745 - accuracy: 0.5 - ETA: 18s - loss: 1.1752 - accuracy: 0.55 - ETA: 17s - loss: 1.1754 - ac - ETA: 16s - loss: 1.1762 - - ETA: 15s - loss: 1.1750 - accuracy: 0.55 - ETA: 15s - loss: 1.174 - ETA: 14s - loss: 1.1756 - accur - ETA: 13s - loss: 1.1747 - accurac - E - E - ETA: 5s - loss: 1.1742 - ac - ETA: 1s - l\n",
      "Epoch 00028: val_loss improved from 1.15242 to 1.15143, saving model to Model.28-0.5693.hdf5\n",
      "909/908 [==============================] - 63s 69ms/step - loss: 1.1761 - accuracy: 0.5538 - val_loss: 1.1514 - val_accuracy: 0.5693\n",
      "Epoch 29/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.1640 - accuracy: 0.5574\n",
      "Epoch 00029: val_loss improved from 1.15143 to 1.11997, saving model to Model.29-0.5830.hdf5\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.1637 - accuracy: 0.5575 - val_loss: 1.1200 - val_accuracy: 0.5830\n",
      "Epoch 30/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.1462 - accuracy: 0.5637 - ETA: 42s - loss: 1.1543 - ac - ETA: 41s  - ETA: 39s - loss:  - ETA: 37s - loss: 1 - ETA: 36s - loss: 1.1485 - accuracy: 0.56 - ETA: 35s -  - ETA: 33s - l - - ETA: 29s - loss: 1.1506 - accuracy: 0. - ETA: 28s - loss: 1.1495 - - ETA: 27s - loss: 1.1516 - accura - ETA: 27s - loss\n",
      "Epoch 00030: val_loss did not improve from 1.11997\n",
      "909/908 [==============================] - 62s 68ms/step - loss: 1.1464 - accuracy: 0.5636 - val_loss: 1.1551 - val_accuracy: 0.5666\n",
      "Epoch 31/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.1338 - accuracy: 0.5676 - ETA: 58s - loss: 1.1723 - accura - ETA: 57s - loss: 1.1419 - accuracy:  - ETA: 57s - loss: 1.1258 - - ETA:  - ETA: 50s - loss: 1.1397 - accuracy: 0. - ETA: 50s - loss: 1.13\n",
      "Epoch 00031: val_loss improved from 1.11997 to 1.10426, saving model to Model.31-0.5851.hdf5\n",
      "909/908 [==============================] - 63s 69ms/step - loss: 1.1340 - accuracy: 0.5675 - val_loss: 1.1043 - val_accuracy: 0.5851\n",
      "Epoch 32/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.1211 - accuracy: 0.5740 - ETA: 57s - loss: 1.1316 - accuracy:  - ETA: 57s - loss: - ETA: 55s - loss: 1. - ETA: 54s - loss: - ETA: 52s - loss: 1. - ETA: 50s - loss: 1.1095 - accuracy: 0 - ETA: 50s - loss: 1.105  - ETA: 46s - loss: 1.1057 - accurac - ETA: 45s - loss - ETA: 43s - loss: 1.1124 - accuracy: 0. - ETA: 4 - ETA: 32s - loss: - ETA: 28s - loss: 1.1179 - accuracy: 0. - ETA: 28s - loss: 1.1183 - accuracy: 0.5 - ETA: 27s - loss: 1. - ETA: 26s - loss: 1.1181 - accuracy: 0.5 - ETA: 26s - loss: 1.1171 - accuracy: 0. - ETA: 25s - loss: 1.1180  - ETA: 24s - loss: 1.1195 - accuracy: 0. - ETA: 24s -  - ETA: 22s - - ETA: 20s - loss: 1.1231 - accura\n",
      "Epoch 00032: val_loss did not improve from 1.10426\n",
      "909/908 [==============================] - 63s 69ms/step - loss: 1.1211 - accuracy: 0.5740 - val_loss: 1.1151 - val_accuracy: 0.5759\n",
      "Epoch 33/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.1073 - accuracy: 0.5785\n",
      "Epoch 00033: val_loss improved from 1.10426 to 1.10131, saving model to Model.33-0.5842.hdf5\n",
      "909/908 [==============================] - 64s 71ms/step - loss: 1.1072 - accuracy: 0.5785 - val_loss: 1.1013 - val_accuracy: 0.5842\n",
      "Epoch 34/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0978 - accuracy: 0.5826\n",
      "Epoch 00034: val_loss did not improve from 1.10131\n",
      "909/908 [==============================] - 64s 71ms/step - loss: 1.0977 - accuracy: 0.5827 - val_loss: 1.1260 - val_accuracy: 0.5653\n",
      "Epoch 35/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0825 - accuracy: 0.5882\n",
      "Epoch 00035: val_loss did not improve from 1.10131\n",
      "909/908 [==============================] - 64s 71ms/step - loss: 1.0825 - accuracy: 0.5882 - val_loss: 1.1370 - val_accuracy: 0.5783\n",
      "Epoch 36/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0753 - accuracy: 0.5923 ETA: 0s - loss: 1.0746 \n",
      "Epoch 00036: val_loss improved from 1.10131 to 1.07999, saving model to Model.36-0.5879.hdf5\n",
      "909/908 [==============================] - 64s 71ms/step - loss: 1.0751 - accuracy: 0.5924 - val_loss: 1.0800 - val_accuracy: 0.5879\n",
      "Epoch 37/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0643 - accuracy: 0.5968\n",
      "Epoch 00037: val_loss did not improve from 1.07999\n",
      "909/908 [==============================] - 65s 71ms/step - loss: 1.0643 - accuracy: 0.5969 - val_loss: 1.0883 - val_accuracy: 0.5907\n",
      "Epoch 38/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0592 - accuracy: 0.5957\n",
      "Epoch 00038: val_loss did not improve from 1.07999\n",
      "909/908 [==============================] - 64s 71ms/step - loss: 1.0590 - accuracy: 0.5958 - val_loss: 1.0804 - val_accuracy: 0.6043\n",
      "Epoch 39/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0480 - accuracy: 0.6013\n",
      "Epoch 00039: val_loss improved from 1.07999 to 1.05312, saving model to Model.39-0.6121.hdf5\n",
      "909/908 [==============================] - 64s 70ms/step - loss: 1.0483 - accuracy: 0.6012 - val_loss: 1.0531 - val_accuracy: 0.6121\n",
      "Epoch 40/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.6105 ETA: 6s - loss: 1.0392 - accuracy:  - ETA: 6s - loss: 1.0394 - accura - ETA: 6s - loss: 1.038 - ETA: 5s - loss: 1.0369 - accu - ETA:  - ETA: 1s - loss: 1.0358 -  - ETA: 0s - loss: 1.0350 - \n",
      "Epoch 00040: val_loss did not improve from 1.05312\n",
      "909/908 [==============================] - 63s 69ms/step - loss: 1.0353 - accuracy: 0.6105 - val_loss: 1.0817 - val_accuracy: 0.6037\n",
      "Epoch 41/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0283 - accuracy: 0.6099\n",
      "Epoch 00041: val_loss did not improve from 1.05312\n",
      "909/908 [==============================] - 64s 70ms/step - loss: 1.0285 - accuracy: 0.6098 - val_loss: 1.0573 - val_accuracy: 0.6074\n",
      "Epoch 42/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0217 - accuracy: 0.6141\n",
      "Epoch 00042: val_loss improved from 1.05312 to 1.03201, saving model to Model.42-0.6229.hdf5\n",
      "909/908 [==============================] - 63s 70ms/step - loss: 1.0216 - accuracy: 0.6141 - val_loss: 1.0320 - val_accuracy: 0.6229\n",
      "Epoch 43/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0140 - accuracy: 0.6123\n",
      "Epoch 00043: val_loss improved from 1.03201 to 1.02971, saving model to Model.43-0.6167.hdf5\n",
      "909/908 [==============================] - 64s 70ms/step - loss: 1.0137 - accuracy: 0.6125 - val_loss: 1.0297 - val_accuracy: 0.6167\n",
      "Epoch 44/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 1.0064 - accuracy: 0.6184\n",
      "Epoch 00044: val_loss did not improve from 1.02971\n",
      "909/908 [==============================] - 65s 71ms/step - loss: 1.0063 - accuracy: 0.6185 - val_loss: 1.1060 - val_accuracy: 0.5904\n",
      "Epoch 45/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9989 - accuracy: 0.6220\n",
      "Epoch 00045: val_loss did not improve from 1.02971\n",
      "909/908 [==============================] - 64s 70ms/step - loss: 0.9990 - accuracy: 0.6220 - val_loss: 1.0656 - val_accuracy: 0.6034\n",
      "Epoch 46/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9883 - accuracy: 0.6248\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.02971\n",
      "909/908 [==============================] - 64s 70ms/step - loss: 0.9882 - accuracy: 0.6248 - val_loss: 1.0314 - val_accuracy: 0.6198\n",
      "Epoch 47/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9766 - accuracy: 0.6302\n",
      "Epoch 00047: val_loss improved from 1.02971 to 1.02012, saving model to Model.47-0.6251.hdf5\n",
      "909/908 [==============================] - 64s 71ms/step - loss: 0.9767 - accuracy: 0.6302 - val_loss: 1.0201 - val_accuracy: 0.6251\n",
      "Epoch 48/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9763 - accuracy: 0.6304\n",
      "Epoch 00048: val_loss did not improve from 1.02012\n",
      "909/908 [==============================] - 66s 72ms/step - loss: 0.9763 - accuracy: 0.6304 - val_loss: 1.0548 - val_accuracy: 0.6118\n",
      "Epoch 49/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9629 - accuracy: 0.6355\n",
      "Epoch 00049: val_loss did not improve from 1.02012\n",
      "909/908 [==============================] - 65s 71ms/step - loss: 0.9627 - accuracy: 0.6355 - val_loss: 1.0220 - val_accuracy: 0.6291\n",
      "Epoch 50/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.6396\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.02012\n",
      "909/908 [==============================] - 65s 71ms/step - loss: 0.9569 - accuracy: 0.6395 - val_loss: 1.0518 - val_accuracy: 0.6201\n",
      "Epoch 51/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9449 - accuracy: 0.6413\n",
      "Epoch 00051: val_loss improved from 1.02012 to 1.01806, saving model to Model.51-0.6297.hdf5\n",
      "909/908 [==============================] - 65s 72ms/step - loss: 0.9450 - accuracy: 0.6414 - val_loss: 1.0181 - val_accuracy: 0.6297\n",
      "Epoch 52/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9423 - accuracy: 0.6428 ETA: 1s - l\n",
      "Epoch 00052: val_loss improved from 1.01806 to 1.00868, saving model to Model.52-0.6334.hdf5\n",
      "909/908 [==============================] - 65s 72ms/step - loss: 0.9422 - accuracy: 0.6427 - val_loss: 1.0087 - val_accuracy: 0.6334\n",
      "Epoch 53/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9367 - accuracy: 0.6466\n",
      "Epoch 00053: val_loss did not improve from 1.00868\n",
      "909/908 [==============================] - 65s 71ms/step - loss: 0.9365 - accuracy: 0.6467 - val_loss: 1.0111 - val_accuracy: 0.6307\n",
      "Epoch 54/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.6502\n",
      "Epoch 00054: val_loss did not improve from 1.00868\n",
      "909/908 [==============================] - 65s 72ms/step - loss: 0.9301 - accuracy: 0.6503 - val_loss: 1.0126 - val_accuracy: 0.6359\n",
      "Epoch 55/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9214 - accuracy: 0.6526\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.00868\n",
      "909/908 [==============================] - 65s 72ms/step - loss: 0.9213 - accuracy: 0.6526 - val_loss: 1.0426 - val_accuracy: 0.6269\n",
      "Epoch 56/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9128 - accuracy: 0.6539\n",
      "Epoch 00056: val_loss did not improve from 1.00868\n",
      "909/908 [==============================] - 65s 72ms/step - loss: 0.9127 - accuracy: 0.6541 - val_loss: 1.0491 - val_accuracy: 0.6307\n",
      "Epoch 57/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9073 - accuracy: 0.6599\n",
      "Epoch 00057: val_loss did not improve from 1.00868\n",
      "909/908 [==============================] - 65s 71ms/step - loss: 0.9075 - accuracy: 0.6598 - val_loss: 1.0359 - val_accuracy: 0.6372\n",
      "Epoch 58/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.9000 - accuracy: 0.6606\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.00868\n",
      "909/908 [==============================] - 65s 72ms/step - loss: 0.8998 - accuracy: 0.6607 - val_loss: 1.0459 - val_accuracy: 0.6229\n",
      "Epoch 59/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.8934 - accuracy: 0.6625\n",
      "Epoch 00059: val_loss did not improve from 1.00868\n",
      "909/908 [==============================] - 65s 72ms/step - loss: 0.8933 - accuracy: 0.6624 - val_loss: 1.0116 - val_accuracy: 0.6356\n",
      "Epoch 60/300\n",
      "908/908 [============================>.] - ETA: 0s - loss: 0.8912 - accuracy: 0.6624\n",
      "Epoch 00060: val_loss did not improve from 1.00868\n",
      "909/908 [==============================] - 65s 72ms/step - loss: 0.8909 - accuracy: 0.6625 - val_loss: 1.0231 - val_accuracy: 0.6322\n",
      "Epoch 00060: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Compliling the model with adam optimixer and categorical crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Specifying parameters for Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=40,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,\n",
    "    zoom_range = 0.05)  # zoom images in range [1 - zoom_range, 1+ zoom_range]\n",
    "\n",
    "datagen.fit(X_train)\n",
    "filepath='Model.{epoch:02d}-{val_accuracy:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir='logs')\n",
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)\n",
    "history = model.fit(datagen.flow(X_train, y_train,\n",
    "                    batch_size=32),\n",
    "                    epochs=300,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    steps_per_epoch=X_train.shape[0]/32,\n",
    "                    callbacks=[lr_reducer, checkpointer, early_stopper, tensorboard]\n",
    "                             )\n",
    "\n",
    "pd.DataFrame(history.history).to_csv(\"history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/3589 [=======>......................] - ETA: 28s"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[1024,256,24,24] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/conv2d_6/Relu (defined at <ipython-input-66-a51f5b03d4ce>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_2094101]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-a51f5b03d4ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model.43-0.6173.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Score: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m               total_epochs=1)\n\u001b[0m\u001b[0;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    636\u001b[0m               *args, **kwds)\n\u001b[0;32m    637\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[1024,256,24,24] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/conv2d_6/Relu (defined at <ipython-input-66-a51f5b03d4ce>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_2094101]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "## To check our best model accuracy and score\n",
    "model2 = load_model('Model.43-0.6173.hdf5')\n",
    "\n",
    "scores = model2.evaluate(np.array(X_test), np.array(y_test), batch_size=1024)\n",
    "print(\"Score: \" + str(scores[0]))\n",
    "print(\"Accuracy: \" + str(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## This method was taken from:\n",
    "##  https://github.com/nhduong/fer2013icprai2018/blob/master/fer2013_ensemble_mlcnns_testing_icprai2018.ipynb\n",
    "test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_datagen.fit(X_test)\n",
    "n_test_ins = X_test.shape[0]\n",
    "test_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "y_pred_ = model2.predict_generator(generator=test_generator, steps=n_test_ins/batch_size, verbose=1)\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title='Unnormalized confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n",
    "        \n",
    "    np.set_printoptions(precision=2)\n",
    "        \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.min() + (cm.max() - cm.min()) / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True expression')\n",
    "    plt.xlabel('Predicted expression')\n",
    "    plt.show()\n",
    "classes=np.array(('angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'))\n",
    "\n",
    "# Predictions\n",
    "y_pred = np.argmax(y_pred_, axis=1)\n",
    "# Ground truth\n",
    "t_te = np.argmax(test_generator.y, axis=1)\n",
    "\n",
    "fig = plot_confusion_matrix(y_test=t_te, y_pred=y_pred,\n",
    "                      classes=classes,\n",
    "                      normalize=True,\n",
    "                      cmap=plt.cm.Greys,\n",
    "                      title='Average accuracy: ' + str(np.sum(y_pred == t_te)/len(t_te)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280 \n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
    "\n",
    "model3 = load_model('fer2013.hdf5')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n",
    "        prediction = model3.predict(cropped_img)\n",
    "        cv2.putText(frame, emotion_dict[int(np.argmax(prediction))], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reference: \n",
    "http://blog.stratospark.com/deep-learning-applied-food-classification-deep-learning-keras.html\n",
    "https://towardsdatascience.com/clustering-with-k-means-1e07a8bfb7ca\n",
    "https://github.com/machrisaa/tensorflow-vgg\n",
    "https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280 \n",
    "https://github.com/nhduong/fer2013icprai2018/blob/master/fer2013_ensemble_mlcnns_testing_icprai2018.ipynb\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
